<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenAI Whisper Speech-to-Text Test</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        .container {
            border: 1px solid #ccc;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }
        button {
            background-color: #4CAF50;
            border: none;
            color: white;
            padding: 10px 20px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 16px;
            margin: 10px 2px;
            cursor: pointer;
            border-radius: 4px;
        }
        button:disabled {
            background-color: #cccccc;
            cursor: not-allowed;
        }
        #recordButton.recording {
            background-color: #f44336;
        }
        pre {
            background-color: #f5f5f5;
            padding: 10px;
            border-radius: 4px;
            overflow-x: auto;
        }
        .visualizer {
            height: 60px;
            background-color: #f0f0f0;
            display: flex;
            align-items: flex-end;
            justify-content: center;
            padding: 10px;
            border-radius: 4px;
            margin: 20px 0;
        }
        .bar {
            width: 10px;
            height: 5px;
            margin: 0 2px;
            background-color: #4CAF50;
            transition: height 0.1s ease;
        }
        #status {
            font-weight: bold;
        }
        .success {
            color: #4CAF50;
        }
        .error {
            color: #f44336;
        }
        #transcriptionResult {
            margin-top: 20px;
            min-height: 100px;
            background-color: #f9f9f9;
            padding: 15px;
            border-radius: 4px;
            border: 1px solid #ddd;
        }
        #apiKeySection {
            margin-bottom: 20px;
        }
        #apiKey {
            width: 100%;
            padding: 8px;
            margin-top: 5px;
            font-family: monospace;
        }
        /* New styles for the color circle */
        .circle-container {
            display: flex;
            flex-direction: column;
            align-items: center;
            margin: 20px 0;
        }
        #colorCircle {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            background-color: gray;
            transition: background-color 0.3s ease;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
            margin: 15px 0;
        }
        #detectedColor {
            font-weight: bold;
            margin-top: 10px;
        }
    </style>
</head>
<body>
    <h1>OpenAI Whisper Speech-to-Text Test</h1>
    <p>This page tests OpenAI's Whisper API for speech-to-text transcription.</p>
    
    <div class="container" id="apiKeySection">
        <h2>1. API Key Configuration</h2>
        <p>Enter your OpenAI API key or leave it as is to use the one from your .env file:</p>
        <input type="text" id="apiKey" placeholder="sk-..." value="">
        <p><small>Note: Your API key is used only for this test and is not stored anywhere beyond your browser.</small></p>
    </div>
    
    <div class="container">
        <h2>2. Record Speech</h2>
        <p>Click the button below to start recording your voice. Click again to stop and send to OpenAI.</p>
        <button id="recordButton">Start Recording</button>
        <div id="status">Ready to record</div>
        
        <div class="visualizer" id="visualizer">
            <!-- Bars will be added with JavaScript -->
        </div>
    </div>
    
    <div class="container">
        <h2>3. Color Circle Test</h2>
        <p>Say a color name (e.g., "red", "blue", "green", "purple") and watch the circle change color!</p>
        <div class="circle-container">
            <div id="colorCircle"></div>
            <div id="detectedColor">Detected color: none</div>
        </div>
    </div>
    
    <div class="container">
        <h2>4. Transcription Result</h2>
        <div id="transcriptionResult">Transcribed text will appear here...</div>
        <div id="debugInfo"></div>
    </div>
    
    <script>
        // Global variables
        let audioContext = null;
        let analyser = null;
        let microphone = null;
        let mediaRecorder = null;
        let audioChunks = [];
        let isRecording = false;
        let animationFrame = null;
        let microphoneStream = null;
        
        // Get the API key from the .env file (will be loaded by parent script)
        const defaultApiKey = "";
        document.getElementById('apiKey').value = defaultApiKey;
        
        // Create the bars for the visualizer
        const visualizer = document.getElementById('visualizer');
        const bars = [];
        const barCount = 20;
        
        for (let i = 0; i < barCount; i++) {
            const bar = document.createElement('div');
            bar.className = 'bar';
            visualizer.appendChild(bar);
            bars.push(bar);
        }
        
        // Function to request microphone access
        async function requestMicrophoneAccess() {
            const statusEl = document.getElementById('status');
            statusEl.className = '';
            statusEl.textContent = 'Requesting microphone access...';
            
            try {
                // Always create a fresh audio context on user interaction
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                } else if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }
                
                // Request microphone access
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                });
                
                // Success! Set up audio processing
                statusEl.textContent = 'Microphone access granted!';
                statusEl.className = 'success';
                
                // Save the stream for later access
                microphoneStream = stream;
                
                // Create analyzer
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 256;
                analyser.smoothingTimeConstant = 0.8;
                
                // Connect microphone to analyzer
                microphone = audioContext.createMediaStreamSource(stream);
                microphone.connect(analyser);
                
                // Start visualization
                startVisualization();
                
                return stream;
            } catch (error) {
                // Handle specific error types
                statusEl.className = 'error';
                
                if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
                    statusEl.textContent = 'Microphone access denied by user or browser settings';
                    console.error('Permission denied:', error);
                } else if (error.name === 'NotFoundError' || error.name === 'DevicesNotFoundError') {
                    statusEl.textContent = 'No microphone found on your device';
                    console.error('No microphone:', error);
                } else if (error.name === 'NotReadableError' || error.name === 'TrackStartError') {
                    statusEl.textContent = 'Microphone is in use by another application';
                    console.error('Microphone in use:', error);
                } else {
                    statusEl.textContent = `Error accessing microphone: ${error.message}`;
                    console.error('Other error:', error);
                }
                
                // Log helpful debugging information
                console.log('TROUBLESHOOTING:');
                console.log('- Browser:', navigator.userAgent);
                console.log('- Error name:', error.name);
                console.log('- Error message:', error.message);
                console.log('- Running on HTTPS?', window.location.protocol === 'https:');
                console.log('- Running on localhost?', window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1');
                
                throw error;
            }
        }
        
        // Function to start audio visualization
        function startVisualization() {
            // Create frequency data array
            const dataArray = new Uint8Array(analyser.frequencyBinCount);
            
            // Animation function
            function animate() {
                // Get frequency data
                analyser.getByteFrequencyData(dataArray);
                
                // Update each bar
                for (let i = 0; i < bars.length; i++) {
                    // Get data for this bar (map from frequency range to bar index)
                    const start = Math.floor(i * dataArray.length / bars.length);
                    const end = Math.floor((i + 1) * dataArray.length / bars.length);
                    
                    // Calculate average frequency value for this range
                    let sum = 0;
                    for (let j = start; j < end; j++) {
                        sum += dataArray[j];
                    }
                    const average = sum / (end - start);
                    
                    // Map to height (5px to 40px)
                    const height = Math.max(5, Math.min(40, average * 0.4));
                    
                    // Set bar height
                    bars[i].style.height = `${height}px`;
                }
                
                // Request next frame
                animationFrame = requestAnimationFrame(animate);
            }
            
            // Start the animation
            animate();
        }
        
        // Function to stop visualization
        function stopVisualization() {
            if (animationFrame) {
                cancelAnimationFrame(animationFrame);
                animationFrame = null;
            }
            
            // Reset bar heights
            bars.forEach(bar => {
                bar.style.height = '5px';
            });
        }
        
        // Function to clean up audio resources
        function cleanupAudioResources() {
            // Stop visualization
            stopVisualization();
            
            // Disconnect microphone
            if (microphone) {
                microphone.disconnect();
                microphone = null;
            }
            
            // Stop all audio tracks
            if (microphoneStream) {
                microphoneStream.getTracks().forEach(track => track.stop());
                microphoneStream = null;
            }
            
            // Close audio context
            if (audioContext) {
                audioContext.close().then(() => {
                    console.log("AudioContext closed");
                    audioContext = null;
                    analyser = null;
                });
            }
        }
        
        // Function to start recording audio
        async function startRecording() {
            try {
                const stream = await requestMicrophoneAccess();
                
                // Reset audio chunks
                audioChunks = [];
                
                // Determine supported MIME type
                let mimeType = 'audio/webm';
                if (!MediaRecorder.isTypeSupported(mimeType)) {
                    mimeType = 'audio/mp4';
                    console.log("Switching to audio/mp4 format as webm is not supported");
                }
                
                // Create media recorder
                mediaRecorder = new MediaRecorder(stream, { mimeType });
                
                // Set recording state
                isRecording = true;
                const recordButton = document.getElementById('recordButton');
                recordButton.textContent = 'Stop Recording';
                recordButton.classList.add('recording');
                
                // Update status
                const statusEl = document.getElementById('status');
                statusEl.textContent = 'Recording...';
                statusEl.className = '';
                
                // Event listener for when data is available
                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };
                
                // Event listener for when recording stops
                mediaRecorder.onstop = () => {
                    isRecording = false;
                    processRecording();
                };
                
                // Start recording
                mediaRecorder.start();
                console.log("Recording started with MIME type:", mediaRecorder.mimeType);
                
            } catch (error) {
                console.error("Error starting recording:", error);
                
                // Update status
                const statusEl = document.getElementById('status');
                statusEl.textContent = `Error: ${error.message}`;
                statusEl.className = 'error';
                
                // Reset recording state
                isRecording = false;
                const recordButton = document.getElementById('recordButton');
                recordButton.textContent = 'Start Recording';
                recordButton.classList.remove('recording');
            }
        }
        
        // Function to stop recording
        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
                
                // Update button
                const recordButton = document.getElementById('recordButton');
                recordButton.textContent = 'Start Recording';
                recordButton.classList.remove('recording');
                
                // Update status
                const statusEl = document.getElementById('status');
                statusEl.textContent = 'Processing audio...';
            }
        }
        
        // Function to process recording and send to OpenAI
        async function processRecording() {
            try {
                // Get API key from input
                const apiKey = document.getElementById('apiKey').value.trim();
                if (!apiKey) {
                    throw new Error("API key is required");
                }
                
                // Create audio blob
                const audioBlob = new Blob(audioChunks, { type: mediaRecorder.mimeType });
                console.log("Audio blob created:", audioBlob.size, "bytes");
                
                // Update status
                const statusEl = document.getElementById('status');
                statusEl.textContent = 'Sending to OpenAI...';
                
                // Create form data with audio file
                const formData = new FormData();
                formData.append('file', audioBlob, 'recording.webm');
                formData.append('model', 'whisper-1');
                
                // Debug info
                const debugInfo = document.getElementById('debugInfo');
                debugInfo.innerHTML = `
                    <p><strong>Sending request with:</strong></p>
                    <ul>
                        <li>API Key: ${apiKey.substring(0, 5)}...${apiKey.substring(apiKey.length - 5)}</li>
                        <li>Audio Size: ${(audioBlob.size / 1024).toFixed(1)} KB</li>
                        <li>MIME Type: ${audioBlob.type}</li>
                    </ul>
                `;
                
                // Send to OpenAI
                const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`
                    },
                    body: formData
                });
                
                // Parse the response
                if (!response.ok) {
                    const errorText = await response.text();
                    throw new Error(`OpenAI API error (${response.status}): ${errorText}`);
                }
                
                const data = await response.json();
                
                // Display the transcription
                const transcriptionResult = document.getElementById('transcriptionResult');
                transcriptionResult.textContent = data.text || 'No transcription returned';
                
                // Update status
                statusEl.textContent = 'Transcription complete!';
                statusEl.className = 'success';
                
                // Update color circle with improved color detection
                const text = data.text.toLowerCase();
                let detectedColor = null;
                
                // Dictionary of color phrases to actual CSS colors
                const colorMap = {
                    // Basic colors
                    'red': 'red',
                    'blue': 'blue',
                    'green': 'green',
                    'yellow': 'yellow',
                    'orange': 'orange',
                    'purple': 'purple',
                    'pink': 'pink',
                    'brown': 'brown',
                    'black': 'black',
                    'white': 'white',
                    'gray': 'gray',
                    'grey': 'gray',
                    
                    // More specific colors
                    'light blue': 'lightblue',
                    'dark blue': 'darkblue',
                    'light green': 'lightgreen',
                    'dark green': 'darkgreen',
                    'light red': 'salmon',
                    'dark red': 'darkred',
                    'hot pink': 'hotpink',
                    'navy blue': 'navy',
                    'navy': 'navy',
                    'teal': 'teal',
                    'cyan': 'cyan',
                    'magenta': 'magenta',
                    'lime': 'lime',
                    'lime green': 'lime',
                    'maroon': 'maroon',
                    'olive': 'olive',
                    'violet': 'violet',
                    'indigo': 'indigo',
                    'turquoise': 'turquoise',
                    'gold': 'gold',
                    'silver': 'silver',
                    'beige': 'beige',
                    'tan': 'tan',
                    'salmon': 'salmon',
                    'coral': 'coral',
                    'crimson': 'crimson',
                    'lavender': 'lavender',
                    'plum': 'plum',
                    'khaki': 'khaki',
                    'sky blue': 'skyblue',
                    'royal blue': 'royalblue',
                    'forest green': 'forestgreen',
                    'sea green': 'seagreen',
                    'olive green': 'olivedrab'
                };
                
                // First try to find exact color matches
                for (const [phrase, color] of Object.entries(colorMap)) {
                    if (text.includes(phrase)) {
                        detectedColor = color;
                        break;
                    }
                }
                
                // If no match found with the map, try a generic regex for CSS color names
                if (!detectedColor) {
                    const colorRegex = /\b(red|green|blue|yellow|purple|orange|pink|black|white|gray|brown|turquoise|silver|gold|beige|teal|coral|lavender|peach|mint|olive|navy|maroon|aquamarine|crimson|fuchsia|khaki|plum|salmon|tan|violet)\b/i;
                    const match = text.match(colorRegex);
                    if (match) {
                        detectedColor = match[0].toLowerCase();
                    }
                }
                
                // Common phrases detection
                if (!detectedColor) {
                    if (text.includes('make it red') || text.includes('color it red') || text.includes('turn it red')) {
                        detectedColor = 'red';
                    } else if (text.includes('make it blue') || text.includes('color it blue') || text.includes('turn it blue')) {
                        detectedColor = 'blue';
                    } else if (text.includes('make it green') || text.includes('color it green') || text.includes('turn it green')) {
                        detectedColor = 'green';
                    }
                    // Add more common phrases as needed
                }
                
                if (detectedColor) {
                    const colorCircle = document.getElementById('colorCircle');
                    const detectedColorText = document.getElementById('detectedColor');
                    
                    // Update circle color with animation
                    colorCircle.style.transition = 'background-color 0.5s ease, transform 0.3s ease';
                    colorCircle.style.backgroundColor = detectedColor;
                    colorCircle.style.transform = 'scale(1.1)';
                    
                    // Reset scale after animation
                    setTimeout(() => {
                        colorCircle.style.transform = 'scale(1)';
                    }, 300);
                    
                    detectedColorText.textContent = `Detected color: ${detectedColor}`;
                    detectedColorText.style.color = detectedColor === 'black' || detectedColor === 'navy' ? '#444' : detectedColor;
                } else {
                    // No color found
                    const detectedColorText = document.getElementById('detectedColor');
                    detectedColorText.textContent = 'No color detected. Try saying a color name like "red" or "blue".';
                    detectedColorText.style.color = 'black';
                }
                
            } catch (error) {
                console.error("Error processing audio:", error);
                
                // Update status
                const statusEl = document.getElementById('status');
                statusEl.textContent = `Error: ${error.message}`;
                statusEl.className = 'error';
                
                // Show error in result area
                const transcriptionResult = document.getElementById('transcriptionResult');
                transcriptionResult.textContent = `Error: ${error.message}`;
                
                // Add debug info
                const debugInfo = document.getElementById('debugInfo');
                debugInfo.innerHTML += `
                    <p class="error"><strong>Error Details:</strong></p>
                    <pre>${error.stack || error.message}</pre>
                `;
            }
        }
        
        // Handle record button click
        document.getElementById('recordButton').addEventListener('click', () => {
            if (isRecording) {
                stopRecording();
            } else {
                startRecording();
            }
        });
        
        // Clean up when leaving the page
        window.addEventListener('beforeunload', () => {
            cleanupAudioResources();
        });
    </script>
</body>
</html>
